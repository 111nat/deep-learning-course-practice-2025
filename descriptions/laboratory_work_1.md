# Лабораторная работа 1: «Метод обратного распространения ошибки»

## Цели работы

**Цель** — изучить метод обратного распространения ошибки для обучения глубоких нейронных сетей на примере двухслойной полносвязной нейронной сети (один скрытый слой).

## Задачи работы

Выполнение лабораторной работы предполагает решение следующих задач:

1. Изучить общую схему работы метода обратного распространения ошибки с использованием стохастического градиентного спуска.
2. Вывести математические формулы для вычисления градиентов функции ошибки по параметрам нейронной сети и формул коррекции весов.
3. Загрузить набор данных [MNIST](https://www.kaggle.com/datasets/hojjatk/mnist-dataset), выполнить предобработку изображений и меток, если это необходимо.
4. Реализовать и протестировать метод обратного распространения ошибки для задачи классификации рукописных цифр из набора данных MNIST.

---

## Конфигурация нейронной сети

1. **Входной слой** содержит `w x h` нейронов, что соответствует разрешению одноканального изображения. Для набора MNIST это составляет `28 x 28`.
2. **Выходной слой** содержит `k` нейронов, что соответствует количеству классов изображений. Для задачи классификации рукописных цифр MNIST — 10 классов.
3. **Скрытый слой** содержит `s` нейронов (параметр).
4. **Параметры метода обучения**:
    - Скорость обучения (learning rate)
    - Размер пачки данных (batch size)
    - Количество эпох
5. **Функции активации**:
    - На скрытом слое — функция ReLU.
    - На выходном слое — функция softmax.
    - Входной слой не содержит функции активации.
6. **Функция ошибки** — кросс-энтропия. ***softmax вместе с кросс-энтропией упрощает вывод формул***.
7. **Контрольные параметры для демонстрации работы нейронной сети**:
    - Размер пачки данных: от 8 до 64 изображений (в зависимости от доступного объема памяти).
    - Скорость обучения: 0.1.
    - Количество скрытых нейронов `s`: 300.
    - Количество эпох: 20.

---

## Требования к результатам выполнения работы

1. **Структура и содержимое Jupyter Notebook**:
    - Необходимо обеспечить демонстрацию избранных изображений и меток классов для подтверждения корректности загрузки и совпадения размерностей.
    - После каждой эпохи обучения модели выводится ошибка классификации на тренировочном наборе данных и время выполнения эпохи.
    - После обучения выводится ошибка классификации на тестовом наборе данных.

2. **Точность классификации**:
    - Для контрольных значений параметров достигнута точность классификации на тестовых данных, сравнимая с точностью, которую выдают стандартные инструменты глубокого обучения (например, Keras или PyTorch).
    - Подсказка: Точность будет ~95% с ***неправильной*** инициализацией весов. В лабораторной работе ожидается результат ***лучше***.

3. **Время выполнения**:
    - Запуск необходимо выполнить в процессе очной сдачи.
    - Время обучения на контрольном наборе параметров не должно превышать время, выделенное на сдачу работы одним студентом (~7–15 минут).

4. **Реализация**:
    - Реализация не должна содержать использование фреймворков глубокого обучения (TensorFlow, Keras, PyTorch и т.д.).
    - Допускается использование библиотек, таких как NumPy, Matplotlib и других классических библиотек.

---

## Оценивание

- **Зачтено / Не зачтено**
